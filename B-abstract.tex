\cleardoublepage
\phantomsection
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
The emergence of the social network Twitter in 2006 resulted in a tremendous boost of the Digital Revolution taking place since the end of the 20th century. As a result, there are on average at least 6000 tweets being tweeted every second. It is estimated that by 2020 the number of users in Twitter will increase up to 275 million users worlwide. Twitter has become so popular that roughly 74\% of all the Twitters' users claim to use the social network as a source to get the daily news. One could say that Twitter has become part of our lives and he would not be far from the truth. By these reasons, this project is committed to develop a classifier with the ability to detect sarcasm in short texts, such as Twitter. \par
Automatic sarcasm detection is the task of detecting sarcasm in a text. Considering the challenges of detecting emotions in a sentiment-bearing text, the approach followed was the design of a~\acf{ml} classifier, with the ability to perform a classification between sarcastic and non-sarcastic tweets.~\acf{nlp} tools played a big role in the task. Moreover, feature extraction facilitated the learning and generalization steps. To ensure optimum~\ac{ml} parameters and provide the best score, several optimization tests were conducted to detect those features which give more information to the classifier. The most noteworthy features to mention were:~\acf{lda}, ngrams and lexical features. Furthermore, a cross validation~\acf{gs} searched over specified parameters to optimize the classifier and get the best score. These consisted of evaluating the F1 score by looping over a set of parameters.  Once the classifier was optimized, the senpy engine was used to provide a framework where a user could insert a short text and the classifier could detect whether or not the text is sarcastic.\par
To conclude, the scores obtained varied between a minimum F1 score of $90.22$\% and a maximum F1 score of $96.68$\%. This difference is because of the learning method used and of the features extracted. As a matter of a fact, the more learning features that the classifiers extracts, the worse the generalization process is. This is due to an overfitting phenomenon.


\vfill
\textbf{Keywords}: Sarcasm, Twitter, Machine Learning, Big Data, Python, NLP, Sarcasm Detection, Sentiments, Emotions and Analysis.