\chapter{Enabling Technologies}
\label{chap:entech}


In this chapter the technologies used to achieve this projects' objective shall be discussed.

\section{Introduction}\par
In this project two main technologies have been used:~\ac{nlp}  and~\ac{ml}. The implementation of these tools has been performed in a combined fashion. Furthermore, other technologies have also been used but they are less relevant than~\ac{nlp} and~\ac{ml}.
On the one hand, with~\ac{nlp} it was possible to transform words into tokens, find syntactical connections between tokens in the same tweet and, finally, construct a numerical vector. On the other hand,~\ac{ml} was very useful to construct a classifier, define a model and feed the vectors from the previous step into the model to make predictions.\par
In short, this is how the project was carried out. Both technologies will be extensively explained in the next sections.

\section{Natural Language Processing}
\ac{nlp} has a crucial role in this project since tests (tweets) is what this project is trying to deal with. Hence, the information that used to train the~\ac{ml} algorithm will come in the form of text. According to~\cite{bird2009natural}, at one extreme~\ac{nlp} could be as simple as counting word frequencies to compare different writing styles. At the other extreme,~\ac{nlp} involves "understanding" complete human utterances, at least to the extent of being able to give useful responses to them.\\
\begin{figure}
	\includegraphics[width=\linewidth]{img/nlp.jpg}
	\caption{Image is taken from~\url{http://www.gartner.com/newsroom/id/2819918}}
	\label{fig:nlptech}
\end{figure}
\Cref{fig:nlptech} shows many emerging technologies that are using \ac{nlp} to provide their services.
After making the reader conscious of the potential of~\ac{nlp}, it will be shown what has been achieved in this project with~\ac{nlp}. \\
Many other examples of \ac{nlp} applications are machine translation, optical character recognition, language detection, topic modeling and many more~\cite{stopwords}.

\label{sec:NLP}
\subsection{Natural Language Toolkit}
\subsubsection{Definition}
\ac{nltk} is a platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength~\ac{nlp} libraries, and an active discussion forum. \footnote{https://www.nltk.org/}.\\
The following applications explained beneath belong to the~\ac{nltk} class.
\subsubsection{Application}
~\ac{nltk} has provided the following tools to this project:
\begin{itemize}
	
	\item \textit{Tokenizer}: A word (Token) is the minimal unit that a machine can understand and process. So any text string cannot be further processed without going through tokenization. Tokenization is the process of splitting the raw string into meaningful tokens~\cite{nltk}. In this project, tokenization was required to get the words containing a tweet. Furthermore, some of the tokens were removed since they do not add meaningful information (e.g. yo, a). These words belong to a stoplist provided by~\ac{nltk} for all supported languages~\cite{nltk}.
	\item \textit{Stemmer}: An stemmer is needed to remove afflixes from a word, ending up with a stem. Stemming is frequently used for indexing words. Instead of storing all forms of a word, the algorithm stores only the stems, greatly reducing the size of index while increasing retrieval accuracy~\cite{nltk}. Particularly, I have chosen the SnowballStemmer because it supports stemming in 13 non-English languages, especially Spanish.
	\item \textit{Part-of-Speech tagging}: \todo[inline]{En tu proyecto no has usado POS porque te fallaba.} A-part-of-Speech tagger (pos\_tag) indicates how a word is functioning within the context of a sentence~\cite{pos}. This part is crucial since a word can function in multiple ways and we would like to distinguish those cases. POS\_tagging has been achieved by collecting each word feature (i.e. noun, verb, adj, adv) and building up a dictionary with the word features' stats.
\end{itemize}

The applications at the top of this section are more commonly known as the preprocessing part previously needed to any \ac{ml} application. Since there are plenty of tasks, they could be easily coordinated by using pipelines.

\section{~\acl{ml}}
When it comes to defining models capable of learning from a text-based source, the two dominant technologies are Natural Language Processing and~\ac{ml}. These two main technologies are concerned in how does the machine process human text. \\
Given data and the proper statistical techniques,~\ac{ml} automates analytical model building capable of recognizing patterns with the aim of making predictions and learning from the model. To be able to do so, a learning process is required.~\ac{ml} algorithms can be classified in three categories~\cite{dataclust}:
\begin{itemize}
	\item \textit{Regression}: Regression learning is predicting a continous varying variable.
	\item \textit{Classification}: Classification is predicting a discrete class label.
	\item \textit{Clustering}: Data clustering is the task of labelling unlabeled data, creating clusters with different properties. 
\end{itemize} 

Furthermore, depending on the dataset the learning process can be carried out in two different ways:
\begin{itemize}
	\item \textit{Supervised learning:} \label{ml:super}The classifier is trained using a percentage of the total dataset while feeding the rest of the dataset to make predictions and to evaluate how good the classifier is. Particularly, this project will be using supervised learning.
	
	\item \textit{Unsupervised learning:} These algorithms base their learning properties on learning patterns present on the dataset. Unlike in supervised learning, there is no training and test dataset partition
\end{itemize}
Two major problems encountered in a~\ac{ml} appication are underfitting and overfitting. These two problems appear due to the bias generated when training the model.\\
It turns out that depending on \textit{how much trained} the classifier is, it can make more assumptions. A learned function that can perform well enough on unseen data points as well as on the training data is termed a generalizable function~\cite{gs}. A generalizable function is achieved through a generalizing fit.
\begin{itemize}
	\item \textit{Underfitting:}
	\begin{figure}
		\includegraphics[width=\linewidth]{img/underfit.png}
		\caption{Underfitting~\cite{gs}}
		\label{fig:underfit}
	\end{figure}
\Cref{fig:underfit} shows that for any point in the diagram, the function always assigns the same predicted value.
	\item \textit{Overfitting:} 
		\begin{figure}
		\includegraphics[width=\linewidth]{img/overfit.png}
		\caption{Overfitting~\cite{gs}}
		\label{fig:overfit}
	\end{figure}
\Cref{fig:overfit} shows that for any point in the diagram, the function performs very good. However, the function has become very unflexible and it will not perform good enough on new points.
\end{itemize}
In contrast with~\ac{ml},~\ac{nlp} is a way to analyze and derive the meaning of human language. To do so,~\ac{nlp} provides a means to process lexical properties as well as syntactic recognition of the human language. However, human language can be rarely precise and ambiguous. Even though~\ac{nlp} has a very wide range of applications, such as automatic text summarizing, relationship extracting, stemming, sentiment analysis and more, it is a difficult problem to tackle in computer science.

\subsection{Sklearn}
\subsubsection{Definition}
Sklearn (a.k.a scikit-learn) is defined \footnote{http://scikit-learn.org/stable/} as a simple and efficient set of tools for data mining and data analysis. It is built in NumPy and SciPy. Sklearn will provide not only the classifiers needed for the pipeline, but also meningful tools.
\subsubsection{Application}
Sklearn has provided the following tools to this project:
\begin{itemize}
	\item \textit{Train and Test splitting}: This method is responsible for splitting the main dataset into two parts, one for training the model and the other one for testing the model. It is a very common~\ac{ml} technique. I chose a test set size of 25 \% of the main dataset. In addition, testing set will later be used to compute parameters such as accuracy and f1 score.
	\item \textit{Pipeline}: A pipeline is a set of procedures connected in series, one after the other where the output of one process is the input to the next~\cite{pipeline1}. Image~\ref{fig:pipeline}\footnote{\url{https://www.slideshare.net/databricks/dataframes-and-pipelines}} represents the idea of a pipeline, though it does not resemble exactly this projects' pipeline.
	\item \textit{Tf-idf-Vectorizer}: The Tf-idfVectorizer is used for every word. It contains two parts; the \textit{if} part, which represents the word frequency and the \textit{idf} part, which represents inverse document frequency~\cite{tfidf1}. This term represents a words' weight in a text. The vectorizer has been used in the pipeline to assign each word a number. \todo[inline]{Es esto cierto?}
	\item \textit{Count Vectorizer}: The algorithm consists of representing a token by considering how many times it appears in a text~\cite{countvect1}. Moreover, Count Vectorizer has defined the ngram range.
	\item \textit{Multinomial NaiveBayes}: A multinomial distribution useful to model feature vectors where each value represents a number of occurrences of a term (or its relative frequency)~\cite{countvect1}. The MultinomialNB has been used as a classifier fed into the pipeline. Its results can be seen.\todo[inline]{a単ade referencia tabla resultados cuando lo pongas}.
	\item \textit{SVM classifier}: Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier\footnote{\url{https://en.wikipedia.org/wiki/Support_vector_machine}}. The SVM classifier has been  fed into the pipeline as input. Its results can be seen.\todo[inline]{a単ade referencia tabla resultados cuando lo pongas}
	\item \textit{Kneighbors classifier}: Is a non-generalized~\ac{ml} algorithm that computes distances from one point to all the training dataset points and chooses the $k$ nearest points~\cite{knn1}.The KNeighbors classifier has been  fed into the pipeline as input. Its results can be seen.\todo[inline]{a単ade referencia tabla resultados cuando lo pongas}
	\item \textit{Logistic Regression classifier}: Is analogous to multiple linear regression, except the outcome is binary. Various transformations are employed to convert the problem to one in which a linear model can be fit~\cite{lr1}. Its results can be seen.\todo[inline]{a単ade referencia tabla resultados cuando lo pongas}.
\end{itemize}

\begin{figure}
	\includegraphics[width=\linewidth]{img/pipeline1.png}
	\caption{A Pipeline.~\cite{pipeline}}
	\label{fig:pipeline}
\end{figure}

\section{Senpy}
Senpy~\cite{senpy} is a framework for text analysis using linked data. It aims at providing a framework where analysis modules can be easily integrated as plugins and, at the same time, provides the core functionalities (data validation, user interaction, logging, etc).

\subsection{Architecture}
Senpy introduces a modular and dynamic architecture which allows implementing different algorithms in an extensible way, while offering a common interface and offering common services that facilitate development, so developers can focus on implementing new and better algorithms.\par
To do so, the architecture consists of two main modules:
\begin{itemize}
	\item Senpy core: Building block of the service.
	\item Senpy plugins: Analysis algorithms.
\end{itemize}
\begin{figure}
	\includegraphics[width=\linewidth]{img/senpy_architecture.png}
	\caption{Senpy's Architecture~\cite{senpy}}
	\label{fig:senpyarch}
\end{figure}
\Cref{fig:senpyarch} depicts a simplified version of the processes involved in Senpy analysis framework.

\section{Bitter}
\label{sec:bitter}
In order to download the Tweets which integrate partially the dataset, the tool bitter was used. Bitter is able to automate several actions (e.g. downloading Tweets) by using a Python wrapper over Twitter which adds support for several Twitter API actions~\footnote{https://github.com/balkian/bitter}.\\
Bitter could be used after installing it and running commands on the UNIX terminal.
\subsection{Architecture}
\Cref{fig:bitter} explains how was bitter used in this project. On the left, the reader can see the information provided to the bitter tool. The configuration file is needed to download the tweets, otherwise nothing will be downloaded. The file contains the user credentials for accessing the Twitter database.\\ The tweet id file is a \textit{.csv} database containing in one column the desired tweets. Finally, bitter will download in \textit{.json} format all the tweets in the indicated folder (indicated on the command). For those tweets that, by any reason, could not be downloaded, a folder named \textit{error} will be created containing the tweet ids that failed.
\begin{figure}
	\includegraphics[width=\linewidth]{img/bitter.png}
	\caption{Bitter Architecture}
	\label{fig:bitter}
\end{figure}
\section{Sefarad}
Sefarad~\cite{sefarad} is an environment developed to explore, analyse and visualise data. 
\subsection{Architecture}
Sefarad is divided in modules:
\begin{itemize}
	\item Visualization: Represent already processed data by using charts. To do so, several dashboards structure the visualization process.
	\item ElasticSearch: Is the persistence layer which stores the data needed for the visualization process.
\end{itemize}
\begin{figure}
	\includegraphics[width=\linewidth]{img/sefarad_architecture.png}
	\caption{Sefarad Architecture~\cite{sefarad}}
	\label{fig:sefaradarch}
\end{figure}
The architecture can be easier understood in the~\cref{fig:sefaradarch}
\section{Elasticsearch}
Elasticsearch is a highly scalable open source search engine with a very powerful analytical engine. The data stored in Elasticsearch is \ac{json} formatted. The primary way of interacting with Elasticsearch is via \textbf{REST API}~\cite{elastic1}.
\subsection{Architecture}
This subsection will not explain into detail Elasticsearch. On contrast, a brief description of the architecture will be described and some of the most common terms will also be mentioned.\par
Even though Elasticsearch is a search engine, it can be understood as a relational database where an index in Elasticsearch is similar to a database that consists of multiple types~\cite{elastic2}. \Cref{fig:elastic1} shows the idea behind.
\begin{figure}
	\includegraphics[width=\linewidth]{img/elastic_1.png}
	\caption{Elasticsearch as a Database~\cite{elastic2}}
	\label{fig:elastic1}
\end{figure}
The most common terms used in Elasticsearch are:
\begin{itemize}
	\item \textit{Node}: An instance of Elasticsearch running on a machine.
	\item \textit{Cluster}: The name under which one or more nodes are connected.
	\item \textit{Document}: A \ac{json} object containing the actual data in key-value pairs.
	\item \textit{Index}: Logical namespace in which Elasticsearch stores data.
	\item \textit{Doc types}: A class of similar documents. A type consists of a name and a mapping.
	\item \textit{Shard}: Containers which can be stored on a single node or multiple nodes. A  shard can be either primary or secondary. A primary shard is the one where all the operations that change the index are directed. A secondary shard is the one that contains duplicate data of the primary shard and helps in quickly searching the data as well as for high availability; in a case where the machine that holds the primary shard goes down, then the secondary shard becomes the primary automatically.
	\item \textit{Replica}: Duplicate copy of the data living in a shard for high availability.
\end{itemize}





