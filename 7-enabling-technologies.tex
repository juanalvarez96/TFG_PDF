	\chapter{Enabling Technologies}
\label{chap:architecture}

\label{sec:introduction}
In this chapter the technologies used to achieve this projects' objective shall be discussed.

\section{Introduction}\par
\todo[inline]{Metemos como hemos sacado los tweets?}
In this project two main technologies have been used:~\ac{nlp}  and ~\ac{ml}. The implementation of these tools has been performed in a combined fashion. \par
\ac{nlp} has allowed me to transform words into tokens, find syntactical connections between tokens in the same tweet and, finally, construct a numerical vector. ~\ac{ml} granted me a way to construct a classifier, define a model and feed the vectors from the previous step into the model to make predictions.\par
In short, this is how the project was carried out. Both technologies will be extensively explained in the next sections.

\section{Natural Language Processing}
\ac{nlp} has a crucial role in this project since what I am analyzing are Tweets, which contain sentences. Hence, the information that I will use to train the ~\ac{ml} algorithm will come in the form of text. According to~\cite{bird2009natural}, at one extreme~\ac{nlp} could be as simple as counting word frequencies to compare different writing styles. At the other extreme,~\ac{nlp} involves "understanding" complete human utterances, at least to the extent of being able to give useful responses to them.\par
After mentioning its importance, I will clarify what libraries have I used and how.
\label{sec:NLP}
\subsection{Natural Language Toolkit}
\subsubsection{Definition}
\ac{nltk} is a platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength~\ac{nlp} libraries, and an active discussion forum. \footnote{https://www.nltk.org/}
\subsubsection{Application}
~\ac{nltk} has provided the following tools to this project:
\begin{itemize}
	\item \textit{Stemmer}: An stemmer is needed to remove afflixes from a word, ending up with a stem. Stemming is frequently used for indexing words. Instead of storing all forms of a word, the algorithm stores only the stems, greatly reducing the size of index while increasing retrieval accuracy~\cite{nltk}. Particularly, I have chosen the SnowballStemmer because it supports stemming in 13 non-English languages, especially Spanish.
	\item \textit{Tokenizer}: A word (Token) is the minimal unit that a machine can understand and process. So any text string cannot be further processed without going through tokenization. Tokenization is the process of splitting the raw string into meaningful tokens~\cite{nltk}. In this project, tokenization was required to get the words containing a tweet. Furthermore, some of the tokens were removed since they do not add meaningful information (e.g. yo, a). These words belong to a stoplist provided by~\ac{nltk} for all supported languages~\cite{nltk}.
	\item \textit{Part-of-Speech tagging}: \todo[inline]{En tu proyecto no has usado POS porque te fallaba.} A-part-of-Speech tagger (pos\_tag) indicates how a word is functioning within the context of a sentence~\cite{pos}. This part is crucial since a word can function in multiple ways and we would like to distinguish those cases. POS\_tagging has been achieved by collecting each word feature (i.e. noun, verb, adj, adv) and building up a dictionary with the word features' stats.
\end{itemize}

These tasks were coordinated by using pipelines. \todo[inline]{No me termina de convencer este párrafo. Qué quiero decir?}

\section{~\acl{ml}}
When it comes to defining models capable of learning from a text-based source, the two dominant technologies are Natural Language Processing and ~\ac{ml}. These two main technologies are concerned in how does the machine process human text. \\
Given data and the proper statistical techniques, ~\ac{ml} automates analytical model building capable of recognizing patterns with the aim of making predictions and learning from the model. To be able to do so, a learning process is required. The learning process can be classified in three~\cite{mltypes}:
\begin{itemize}
	\item \textit{Supervised learning}: Widely used for classification applications. A model is trained to make predictions and is corrected whenever the model makes a mistake.
	\item \textit{Unsupervised learning}: Extract general rules. A model deduces recognizable structures in the input data-set. Result is unknown  
	\item \textit{Semi-supervised learning}: Part of the data-set result is known. 
\end{itemize} 

In contrast with ~\ac{ml},~\ac{nlp} is a way to analyze and derive the meaning of human language. To do so, it provides a means to process lexical properties as well as syntactic recognition of the human language. However, human language can be rarely precise and ambiguous. Even though Natural Language Processing has a very wide range of applications, such as automatic text summarizing, relationship extracting, stemming, sentiment analysis and more, it is a difficult problem to tackle in computer science.

\subsection{Sklearn}
\subsubsection{Definition}
Sklearn (a.k.a scikit-learn) is defined \footnote{http://scikit-learn.org/stable/} as a simple and efficient set of tools for data mining and data analysis. It is built in NumPy and SciPy. Sklearn will provide not only the classifiers needed for the pipeline, but also meningful tools.
\subsubsection{Application}
Sklearn has provided the following tools to this project:
\begin{itemize}
	\item \textit{Train and Test splitting}: This method is responsible for splitting the main dataset into two parts, one for training the model and the other one for testing the model. It is a very common ~\ac{ml} technique. I chose a test set size of 25 \% of the main dataset. In addition, testing set will later be used to compute parameters such as accuracy and f1 score.
	\item \textit{Pipeline}: A pipeline is a set of procedures connected in series, one after the other where the output of one process is the input to the next~\cite{pipeline1}. Image~\ref{fig:pipeline}\footnote{\url{https://www.slideshare.net/databricks/dataframes-and-pipelines}} represents the idea of a pipeline, though it does not resemble exactly this projects' pipeline.
	\item \textit{Tf-idf-Vectorizer}: The Tf-idfVectorizer is used for every word. It contains two parts; the \textit{if} part, which represents the word frequency and the \textit{idf} part, which represents inverse document frequency~\cite{tfidf1}. This term represents a words' weight in a text. The vectorizer has been used in the pipeline to assign each word a number. \todo[inline]{Es esto cierto?}
	\item \textit{Count Vectorizer}: The algorithm consists of representing a token by considering how many times it appears in a text~\cite{countvect1}. Moreover, Count Vectorizer has defined the ngram range.
	\item \textit{Multinomial NaiveBayes}: A multinomial distribution useful to model feature vectors where each value represents a number of occurrences of a term (or its relative frequency)~\cite{countvect1}. The MultinomialNB has been used as a classifier fed into the pipeline. Its results can be seen.\todo[inline]{añade referencia tabla resultados cuando lo pongas}.
	\item \textit{SVM classifier}: Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier\footnote{\url{https://en.wikipedia.org/wiki/Support_vector_machine}}. The SVM classifier has been  fed into the pipeline as input. Its results can be seen.\todo[inline]{añade referencia tabla resultados cuando lo pongas}
	\item \textit{Kneighbors classifier}: Is a non-generalized ~\ac{ml} algorithm that computes distances from one point to all the training dataset points and chooses the $k$ nearest points~\cite{knn1}.The KNeighbors classifier has been  fed into the pipeline as input. Its results can be seen.\todo[inline]{añade referencia tabla resultados cuando lo pongas}
	\item \textit{Logistic Regression classifier}: Is analogous to multiple linear regression, except the outcome is binary. Various transformations are employed to convert the problem to one in which a linear model can be fit~\cite{lr1}. Its results can be seen.\todo[inline]{añade referencia tabla resultados cuando lo pongas}.
\end{itemize}

\begin{figure}
	\includegraphics[width=\linewidth]{img/pipeline1.png}
	\caption{A Pipeline.~\cite{pipeline}}
	\label{fig:pipeline}
\end{figure}

\section{Senpy}
Senpy~\cite{senpy} is a framework for text analysis using linked data. It aims at providing a framework where analysis modules can be easily integrated as plugins and, at the same time, provides the core functionalities (data validation, user interaction, logging, etc).

\subsection{Architecture}
Senpy introduces a modular and dynamic architecture which allows implementing different algorithms in an extensible way, while offering a common interface and offering common services that facilitate development, so developers can focus on implementing new and better algorithms.\par
To do so, the architecture consists of two main modules:
\begin{itemize}
	\item Senpy core: Building block of the service.
	\item Senpy plugins: Analysis algorithms.
\end{itemize}
\begin{figure}
	\includegraphics[width=\linewidth]{img/senpy_architecture.png}
	\caption{Senpy's Architecture~\cite{senpy}}
	\label{fig:senpyarch}
\end{figure}
\Cref{fig:senpyarch} depicts a simplified version of the processes involved in Senpy analysis framework.

\section{Luigi}
The purpose of Luigi is to run a set of pipelined processes to perform several tasks~\cite{luigi}. In that sense, Luigi will help stitching the tasks together and tends to the workflow management. Luigi is written in Python.\par
\subsection{Architecture}
There are two fundamental building blocks in Luigi- the \textit{Task} class and \textit{Target} class:
\begin{itemize}
	\item \textit{Target}: This class corresponds to a file on a disk, or some kind of checkpoint that will be used as the output of a task.
	\item \textit{Task}: This class is where the computation is done. The \textit{Tasks} consume \textit{Targets} that were created by some other task. Each task define outputs by using the \textit{output()} method. 
\end{itemize}
The behaviour of Luigi is briefly depicted in the \cref{fig:luigytask}.
\begin{figure}
	\includegraphics[width=\linewidth]{img/luigi_tasks.png}
	\caption{Task and Target illustration.~\cite{luigi}}
	\label{fig:luigytask}
\end{figure}
\section{Bitter}
In order to download the Tweets which integrate my datataset I used Bitter. Bitter is able to automate several actions (e.g. downloading Tweets) by using a Python wrapper over Twitter which adds support for several Twitter API actions~\footnote{https://github.com/balkian/bitter}.
\section{Sefarad}
Sefarad~\cite{sefarad} is an environment developed to explore, analyse and visualise data. 
\subsection{Architecture}
Sefarad is divided in modules:
\begin{itemize}
	\item Visualization: Represent already processed data by using charts. To do so, several dashboards structure the visualization process.
	\item ElasticSearch: Is the persistence layer which stores the data needed for the visualization process.
\end{itemize}
\begin{figure}
	\includegraphics[width=\linewidth]{img/sefarad_architecture.png}
	\caption{Sefarad Architecture~\cite{sefarad}}
	\label{fig:sefaradarch}
\end{figure}
The architecture can be easier understood in the~\cref{fig:sefaradarch}
\section{Elasticsearch}
Elasticsearch is a highly scalable open source search engine with a very powerful analytical engine. The data stored in Elasticsearch is \ac{json} formatted. The primary way of interacting with Elasticsearch is via \textbf{REST API}~\cite{elastic1}.
\subsection{Architecture}
This subsection will not explain into detail Elasticsearch. On contrast, a brief description of the architecture will be described and some of the most common terms will also be mentioned.\par
Even though Elasticsearch is a search engine, it can be understood as a relational database where an index in Elasticsearch is similar to a database that consists of multiple types~\cite{elastic2}. \Cref{fig:elastic1} shows the idea behind.
\begin{figure}
	\includegraphics[width=\linewidth]{img/elastic_1.png}
	\caption{Elasticsearch as a Database~\cite{elastic2}}
	\label{fig:elastic1}
\end{figure}
The most common terms used in Elasticsearch are:
\begin{itemize}
	\item \textit{Node}: An instance of Elasticsearch running on a machine.
	\item \textit{Cluster}: The name under which one or more nodes are connected.
	\item \textit{Document}: A \ac{json} object containing the actual data in key-value pairs.
	\item \textit{Index}: Logical namespace in which Elasticsearch stores data.
	\item \textit{Doc types}: A class of similar documents. A type consists of a name and a mapping.
	\item \textit{Shard}: Containers which can be stored on a single node or multiple nodes. A  shard can be either primary or secondary. A primary shard is the one where all the operations that change the index are directed. A secondary shard is the one that contains duplicate data of the primary shard and helps in quickly searching the data as well as for high availability; in a case where the machine that holds the primary shard goes down, then the secondary shard becomes the primary automatically.
	\item \textit{Replica}: Duplicate copy of the data living in a shard for high availability.
\end{itemize}





